{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19edcc3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from datasets import ClassLabel, load_dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5035f51e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/eng.word.train.tsv',sep='\\t',header=None)\n",
    "dev = pd.read_csv('../data/eng.word.dev.tsv',sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f8ae19",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train = get_bio_tags(train)\n",
    "dev = get_bio_tags(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa056f96",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train['len'] = [len(str(w)) for w in train[0]]\n",
    "dev['len'] = [len(str(w)) for w in dev[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6192f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dev['bio_tokens'] = tokenize_word_from_bio(dev[0], dev.bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcbde5f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(train),len(dev))\n",
    "train = train[train.len < 30]\n",
    "dev = dev[dev.len < 30]\n",
    "\n",
    "print(len(train),len(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734c5cf8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "medical_subwords = pd.read_csv('../data/dev_bio_words.tsv',header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301ffc6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "medical_subwords[0] = medical_subwords['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b00e95",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bio_dev = dev.merge(medical_subwords,on=0, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b9f374",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def bio_with_tokenizer(dev, tokenizer_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    if tokenizer_name == 't5-base':\n",
    "        dev['{}'.format(tokenizer_name)] = [' '.join(tokenizer.tokenize(w))[1:].split() for w in dev[0]]\n",
    "        dev['{}'.format(tokenizer_name)] = [' '.join([p[0]] + ['@@' + n for n in p[1:]]) for p in dev['{}'.format(tokenizer_name)]]\n",
    "    else:\n",
    "        dev['{}'.format(tokenizer_name)] = [' '.join(tokenizer.tokenize(w)).replace('##','@@') for w in dev[0]]\n",
    "        \n",
    "    dev['{}_bio'.format(tokenizer_name)] = get_bio_tag_col(dev[0], dev['{}'.format(tokenizer_name)])\n",
    "    dev['{}_len'.format(tokenizer_name)] = [len(w.split()) for w in dev['{}'.format(tokenizer_name)]]\n",
    "    \n",
    "    return dev, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39301ab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "toks = ['bert-base-uncased','microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract','osunlp/BioVocabBERT']\n",
    "\n",
    "for tok in toks:\n",
    "    bio_dev, _ = bio_with_tokenizer(bio_dev, tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1262b8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bio_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda1fefb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from evaluation.evaluate_mod import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b6aee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bio_dev[[0,1]].to_csv('../output/gold.dev.tsv',header=None,index=None,sep='\\t')\n",
    "\n",
    "for tok in toks:\n",
    "    print(tok)\n",
    "    bio_dev[[0,tok]].to_csv('../output/pred.dev.tsv',header=None,index=None,sep='\\t')\n",
    "    \n",
    "    evaluate_inline('../output/gold.dev.tsv','../output/pred.dev.tsv', False)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e1df1a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Character-Based LM Tokenizer Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b19dd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "best_f1 = 0\n",
    "best_index = None\n",
    "\n",
    "for folder in glob('canine_exps/*'):\n",
    "    results = pickle.load(open('{}/results.p'.format(folder),'rb'))\n",
    "    \n",
    "    max_f1 = np.max([r[0]['overall_f1'] for r in results])\n",
    "    max_f1_ind = np.argmax([r[0]['overall_f1'] for r in results])\n",
    "    \n",
    "    if best_f1 < max_f1:\n",
    "        best_f1 = max_f1\n",
    "        \n",
    "        best_index = (folder, max_f1_ind)\n",
    "        \n",
    "best_model = AutoModelForTokenClassification.from_pretrained('{}/best_model'.format(best_index[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07830141",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google/canine-c')\n",
    "bio_dev['canine'] = [tokenize_phrase(phrase, best_model) for phrase in tqdm(bio_dev[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a43336",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bio_dev['canine'] = [phrase.replace('##','@@') for phrase in bio_dev['canine']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf73bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bio_dev[[0,1]].to_csv('../output/gold.dev.tsv',header=None,index=None,sep='\\t')\n",
    "\n",
    "for tok in ['canine']:\n",
    "    print(tok)\n",
    "    bio_dev[[0,tok]].to_csv('../output/pred.dev.tsv',header=None,index=None,sep='\\t')\n",
    "    \n",
    "    evaluate_inline('../output/gold.dev.tsv','../output/pred.dev.tsv', False)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c99202",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sub-opt",
   "language": "python",
   "name": "sub-opt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}